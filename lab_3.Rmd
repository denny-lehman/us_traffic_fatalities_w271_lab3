---
title: 'Lab 3: Panel Models'
subtitle: 'US Traffic Fatalities: 1980 - 2004'
output: 
  bookdown::pdf_document2: default
---

```{r load packages,  message=FALSE}
library(tidyr)
library(dplyr)
library(ggrepel)
library(ggthemes)
library(stargazer)
library(gridExtra)
library(plm)
library(knitr)
library(patchwork)
library(lubridate)
library(tsibble)
library(tseries)
```


# U.S. traffic fatalities: 1980-2004

In this lab, we are asking you to answer the following **causal** question: 

> **"Do changes in traffic laws affect traffic fatalities?"**  

To answer this question, please complete the tasks specified below using the data provided in `data/driving.Rdata`. This data includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is also provided in the dataset. 

```{r load data, echo = TRUE}
load(file="./data/driving.RData")

## please comment these calls in your work 
# glimpse(data)
# desc
```


# (30 points, total) Build and Describe the Data 

1. (5 points) Load the data and produce useful features. Specifically: 
    - Produce a new variable, called `speed_limit` that re-encodes the data that is in `sl55`, `sl65`, `sl70`, `sl75`, and `slnone`; 
    - Produce a new variable, called `year_of_observation` that re-encodes the data that is in `d80`, `d81`, ... , `d04`. 
    - Produce a new variable for each of the other variables that are one-hot encoded (i.e. `bac*` variable series). 
    - Rename these variables to sensible names that are legible to a reader of your analysis. For example, the dependent variable as provided is called, `totfatrte`. Pick something more sensible, like, `total_fatalities_rate`. There are few enough of these variables to change, that you should change them for all the variables in the data. (You will thank yourself later.)
    
## Answer.1

Please refer to the code below.
    
```{r,  warning=FALSE, message=FALSE, fig.height=3}
year_of_observation = as.matrix(
  data[match("d80", colnames(data)):match("d04", colnames(data))]
  ) %*% c(1980:2004)

data_clean <- data %>% mutate(
  year_of_observation = as.numeric(year_of_observation),
  year = factor(year),
  state = factor(state),
  
  # laws
  speed_limit = factor(
    round(sl55) * 55 + round(sl65) * 65 + round(sl70) * 70 + 
      round(sl75) * 75 + round(slnone) * 100
  ), # assuming 100 for no speed limit
  blood_alcohol_limit = factor(
    round(bac10) * 1 + round(bac08) * 2,
    labels = c('none', 'bac10', 'bac08')
  ),
  zero_tolerance_law = factor(round(zerotol)),
  per_se_law = factor(round(perse)),
  graduated_drivers_license_law = factor(round(gdl)),
  seat_belt = factor(seatbelt,labels=c('none','primary', 'secondary')),
  min_age = as.factor(minage),
  speed_limit_70plus = factor(round(sl70plus)),
  
  # demographics
  log_unemployment_rate = log(unem),
  log_vehicle_miles_per_capita = log(vehicmiles / statepop),
  
  # dependent variable
  log_total_fatalities_rate = log(totfatrte)
    ) %>% 
  rename(
    total_fatalities_rate = totfatrte
    ) %>%
  dplyr::select(
      year,
      state,
      total_fatalities_rate,
      log_total_fatalities_rate,
      seat_belt,
      speed_limit,
      blood_alcohol_limit,
      log_vehicle_miles_per_capita,
      speed_limit_70plus,
      log_unemployment_rate,
      perc14_24,
      zero_tolerance_law,
      graduated_drivers_license_law,
      per_se_law
    )
```

    
2. (5 points) Provide a description of the basic structure of the dataset. What is this data? How, where, and when is it collected? Is the data generated through a survey or some other method? Is the data that is presented a sample from the population, or is it a *census* that represents the entire population? Minimally, this should include:
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 

## Answer.2

The data comes from a study conducted by our textbook author. In the study^[Wooldridge Source: Freeman, D.G. (2007), “Drunk Driving Legislation and Traffic Fatalities: New Evidence on BAC 08 Laws,” Contemporary Economic Policy 25, 293–308.], Wooldridge et al. invistigate the effects of Blood Alcohol Content laws on driving safety in the United States. The data contains observations of each State from 1980 to 2004 which means it includes both longitudinal and cross sectional aspects. Thus, the panel data is the culmination of both discrete and time series statistics and requires panel data analysis. The data was taken during a natural experiment, where the split into treatment and control groups came from state legislatures enacting laws at different times. Specifically, fatality data was aggregated by the Fatality Analysis Reporting System which collects all instances of traffic accidents that result in the death of a vehicle occupant or nonmotorist. The dependent variable is the rate of total traffic fatalities per 100,000 population at the state level.

    
3. (20 points) Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable `total_fatalities_rate` and the potential explanatory variables. Minimally, this should include: 
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
    - What is the average of `total_fatalities_rate` in each of the years in the time period covered in this dataset? 

As with every EDA this semester, the goal of this EDA is not to document your own process of discovery -- save that for an exploration notebook -- but instead it is to bring a reader that is new to the data to a full understanding of the important features of your data as quickly as possible. In order to do this, your EDA should include a detailed, orderly narrative description of what you want your reader to know. Do not include any output -- tables, plots, or statistics -- that you do not intend to write about.


## Answer.3

```{r , warning=FALSE}
# histogram of total fatality rate
p1 <- data %>%
ggplot(aes(x=totfatrte)) + geom_histogram(aes(y=..density..), alpha=0.8) + 
  geom_density(alpha=0.2, fill="cornflowerblue") +
  geom_vline(aes(xintercept=mean(totfatrte)), 
             color="black", linetype="dashed", size=1) +
    labs(x="total_fatalities_rate", 
         y = "Density", 
         title='Fig.1 Untransformed data will be affected by outliers, right tail')

# histogram of log transform
p2<-data_clean %>%
  ggplot(aes(x=log_total_fatalities_rate)) + 
  geom_histogram(aes(y=..density..), alpha=0.8) + 
  geom_density(alpha=0.2, fill="cornflowerblue") +
  geom_vline(aes(xintercept=mean(log_total_fatalities_rate)), 
             color="black", linetype="dashed", size=1) +
    labs(x="log_total_fatalities_rate", 
         y = "Density", 
         title='Fig.2 Log transform of total fatality rate reduces outliers')

p1/p2
```

We start by examining the dependent variable - total fatality rate. The total fatality rate represents the number of deaths from traffic accidents per 100,000 people. The histogram of values shows a mean of about 20 with a long right tail and outlier values near 50. The outliers could lead to a biased model, so we consider a log transform of the dependent variable. The log transform shows a more normal curve and removal of the right tailed outliers. The reduction of outlier effects is a desirable property for regression modeling, so we choose to model the log of total fatality rate, now called `log_total_fatality_rate`.  


```{r, warning=FALSE, message=FALSE, fig.height=4}
# boxplot over time
p1<- data_clean %>% ggplot(aes(reorder(state, desc(
  log_total_fatalities_rate
)), log_total_fatalities_rate,
fill = state)) +
  geom_boxplot(alpha = 0.4) +
  # theme_economist_white(gray_bg=F) +
  theme(legend.position = "none", axis.text.y = element_text(size = 6)) +
  scale_y_continuous() +
  xlab("State ID") +
  ylab("Log Total Fatalities Rate") +
  ggtitle("Fig.3 Boxplot of Log Total Fatalities Rate by state")+
  coord_flip()
p1
```
We continue EDA by investigating the effects of log fatality rates on a state by state basis. The boxplot in Fig.3 shows States have varying degrees of both median and spread of fatality rates over time. Each state will play a role in explaining the dependent variable, especially as a time invariant feature.

```{r warning=FALSE, message=FALSE, fig.height=4}
# lineplot over time
p2<- data_clean %>%
  ggplot(aes(year_of_observation, log_total_fatalities_rate, color = state)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm") +
  facet_wrap( ~ state, scales = "free_y") +
  theme(
    legend.position = "none",
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1,
      size = 6
    ),
    axis.text.y = element_text(size = 6)
  ) +
  theme(strip.text = element_text(size = 4)) +
  scale_y_continuous() +
  ggtitle("Fig.4 Total fatalities rate by state over time")+
  xlab("year of observation") 

p2
```

Next, we consider the log fatality rate over time conditional on each State. Fig.4 shows that fatality rate over time has decreased for almost every state. We can conclude that EDA shows driver fatality rate overall has been trending downward but is different on a state by state basis.

```{r, message=FALSE}
data_clean %>% filter(year==2004) %>% dplyr::select(
    state,
    log_total_fatalities_rate,
    log_unemployment_rate,
    log_vehicle_miles_per_capita,
    perc14_24,
)  %>% 
  group_by(state) %>%summarise_all(mean)%>%dplyr::select(-state) %>%cor()%>%
  kable(caption = "Correlation among continuous variables",digits = 2)

```

Finally, in order to observe the correlations among the continuous variables we are interested in, we average them along time to get the cross-sectional data for each state. As listed in Table.1, the correlation between log_total_fatalities_rate and log_vehicle_miles_per_capita seems strong, but the rest of the relations are not obvious. We will further investigate their relations with panel data models.

# (15 points) Preliminary Model

Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004 and interpret what you observe. In this section, you should address the following tasks: 
- Why is fitting a linear model a sensible starting place? 
- What does this model explain, and what do you find in this model? 
- Did driving become safer over this period? Please provide a detailed explanation.
- What, if any, are the limitation of this model. In answering this, please consider **at least**: 
    - Are the parameter estimates reliable, unbiased estimates of the truth? Or, are they biased due to the way that the data is structured?
    - Are the uncertainty estimate reliable, unbiased estimates of sampling based variability? Or, are they biased due to the way that the data is structured? 
    
## Answer

In this section, we pool the state effects together and investigate total fatality rates over time using a linear model. Linear models are a sensible place to start because they are simple and easy to interpret. In our case, this linear model can show how much the average total fatalities rate changed each year compared to 1980.

```{r}
preliminary_model  <- lm(log_total_fatalities_rate ~ year, data = data_clean)
summary(preliminary_model)
```

This model describes the log of total fatality rate conditional on the year the driving occurred. Since each year is a binary variable, year 1980 represents the intercept term in the model and years 1981 through 2004 are represented with an indicator variable. The coefficients on all non intercept terms represent the change in the log fatality rate compared to the 1980 baseline. 

$$log(total.fatality.rate_{i}) = \beta_{1980} + \Sigma_{i=1981}^{2004}\beta_i*I(year_i)$$


To achieve the total fatality rate, we require the inverse transform for the log function. 

$$ total.fatality.rate_i = exp(\beta_{1980} + \Sigma_{i=1981}^{2004}\beta_i*I(year_i))$$

Initially, our model estimates that the total fatality rate of 1980 is `r round(exp(3.195),2)`. With a p-value near 0, this estimate is statistically significant. The additional beta's of the model are the estimations of each year's impact on the fatality rate from the baseline of 1980. The coefficients over time are negative and decreasing, meaning that the model suggests driving got safer over the period because the fatality rate decreased. Almost all of these estimates, save for 1981 only, are statistically significant. 

```{r, fig.height=3}
#model estimated fatality rate over time
new_data <- data_clean %>% filter(state==1)
pred <- predict(preliminary_model,newdata = new_data)
pred <- exp(pred)

# plot(c(1980:2004), pred)
data.frame(year=c(1980:2004), total_fatality_rate=pred) %>% 
  ggplot() + geom_point(aes(x=year, y=total_fatality_rate)) + 
  geom_smooth(aes(x=year, y=total_fatality_rate)) + 
  labs(title='Preliminary Model estimations for total fatality rate from 1980 to 2004')
```

However, this preliminary model is limited in at least the following three aspects:

**1. Violation of IID assumption.** An important feature of the dataset is that it has both longitudinal (recorded fatality rates for years 1980-2004) and cross sectional (the state at which the driving occurred) features which are hallmarks of panel data. In this way, each state has multiple records in the dataset corresponding to different years, such that Alabama has 24 repeated observations in the dataset, one for each year. This property can lead to violations in the linear model assumptions, specifically that there is no correlation in the residuals. 

We can test the independence of the model residuals using the Ljung-Box test, which has the following hypothesis.

$$H_0: data\ are\ independently\ distributed$$
$$H_A: data\ are\ not\ independently\ distributed$$

```{r}
plot(preliminary_model$residuals, 
     main = 'residuals of the preliminary model vs record index', 
     ylab = 'residuals')

preliminary_model$residuals %>% as.ts() -> resid.ts
Box.test(resid.ts, lag = 1, type = "Ljung-Box")

Box.test(resid.ts, lag = 10, type = "Ljung-Box")

# fails both lag 1 and lag 10 tests for independence

```

The plot above shows the residuals of the preliminary model moving in a wave pattern, a feature of serial correlation. When testing to see if the residuals of the model were independent, we used the Ljung-Box test with the null hypothesis that the data is independent. Based on the test, the p-value is less than 0.05 for both 1 and 10 lags, and we reject the null hypothesis, the residuals are not independent. For this reason, the results of the linear model are not reliable, and we should consider better models for panel data that incorporate both time and cross sectional features, like a fixed effects model.

**2. Omitted variable bias.** The model basically assumes *year* is the only thing impacting the fatality rate and no other other variable that can impact fatality rate, which is not true. Therefore, the estimates are biased.

**3. Relevance to the topic.** As stated at the very begining, the purpose of the research is to answer the causal question: “Do changes in traffic laws affect traffic fatalities?” Obviously this model cannot help answer it.


# (15 points) Expanded Model 

Expand the **Preliminary Model** by adding variables related to the following concepts: 

- Blood alcohol levels 
- Per se laws
- Primary seat belt laws (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)
- Secondary seat belt laws 
- Speed limits faster than 70 
- Graduated drivers licenses 
- Percent of the population between 14 and 24 years old
- Unemployment rate
- Vehicle miles driven per capita. 

If it is appropriate, include transformations of these variables. Please carefully explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. 

- How are the blood alcohol variables defined? Interpret the coefficients that you estimate for this concept. 
- Do *per se laws* have a negative effect on the fatality rate? 
- Does having a primary seat belt law? 

## Answer

The table below summarizes how to reflect the concepts with our data and their transformation. The only explanation needed is the "Round" procedure. The related variables( *bac08*, *bac10*, *perse*, *sl70plus*, *gld*) are conceptually dummy variables. However, we see some fractional number in the raw data, which denotes the amount of time it was in effect (a law starting in June would have 6/12 months = 0.5 value). We engineer these features using rounding such that our new feature shows if the law was effective for the majority of the year.

| Concept                                     | Variable in raw data     | Round | Log trans. | Description                                                                                                 |
| ------------------------------------------- | ------------------------ | ----- | ------------------ | ----------------------------------------------------------------------------------------------------------- |
| Blood alcohol levels                        | bac08, bac10             | yes   |                    | Combine bac08 and bac10 into a ordinal variabe: None=no limit; bac10=limit at 0.1; bac08=limit at 0.08  |
| Per se laws                                 | perse                    | yes   |                    | 0=no per se law, 1=per se law                                                                               |
| Primary seat belt laws                      | seatblet                 |    |                    | 0=no seat belt law; 1=primary, 2=secondary                                                              |
| Secondary seat belt laws                    | as above                 |    |                    | as above                                                                                                    |
| Speed limits faster than 70                 | sl70plus                 | yes   |                    | 0=speed limit <70; 1=speed limit >=70 or no limit                                                       |
| Graduated drivers licenses                  | gdl                      | yes   |                    | 0=no graduated drivers license law                                                                          |
| Percent of the population between 14 and 24 | perc14_24                |       |                    | It's a continuous variable with no obvious skewness, so no transformation needed                         |
| Unemployment rate                           | unem                     |       | yes                | It's a continuous variable and obviously right skewed, so log transformation is preferred                |
| Vehicle miles driven per capita             | vehicmiles, statepop |       | yes                | The ratio of these two variables. Since it's is right-skewed, we perform log transformation |


BAC laws put a limit on alcohol consumption for drivers where BAC is measured in a percentage of alcohol in the blood. Before Congress enacted a national BAC limit in the year 2000, each state was able to enact their own laws with varying legal limits on BAC. Our data set identifies these laws as a state having no BAC limit, a BAC limit of .10, and a BAC limit of .08. We label the state based on the law that was in place for the majority of the year. 

The effect of having no BAC law was absorbed by the intercept term, which also includes the effects of the year 1980, no seatbelt laws, and more. The estimate for having a BAC08 law was -0.062 with a p-value of 0.01. That means that states enacting a 0.08 BAC law expect a decrease in fatality rate of 6.2% all else being equal. States that enact a more lenient level of 0.10 BAC see less of an effect. Those states see a decrease in fatality rates of 1.7%, but with a p-value of 0.34 the effect is not statistically significant.

Per se laws strive to improve driver safety by allowing judges to suspend the license of an arrest drunk driver before they are convicted of a crime. Unfortunately, the pooled model does not support the effect. With a coefficient -0.01 and a p-value 0.19, there is not enough evidence to support per se laws significantly improving safety via log fatality rates.  


Seat belts help reduce injuries and fatalities in car crashes. In an attempt to improve seat belt usage, states enacted primary and secondary seat belt laws. Primary seat belt laws allow law enforcement to stop drivers and give tickets for not wearing a seatbelt. Secondary laws allow police performing a traffic stop for another reason, say speeding, to give out tickets for not wearing a seat belt. Secondary laws would be considered more stringent.

The coefficients for primary and secondary seatbelt laws are 0.009 and 0.02042 respectively. Unfortunately, both p-values are much larger than the critical value of 0.05, with the p-value of primary laws being 0.96 and the p-value of secondary laws being 0.34. There is not enough evidence to suggest that there is a significant relationship between seatbelt laws and log fatality rates, all else equal. 

```{r}
expanded_model = plm(
  log_total_fatalities_rate ~ year + blood_alcohol_limit + per_se_law +
    seat_belt + speed_limit_70plus + graduated_drivers_license_law + perc14_24 +
    log_unemployment_rate + log_vehicle_miles_per_capita,
  data = data_clean,index = c("state", "year"),
  model = "pooling"
)
summary(expanded_model)
```

# (15 points) State-Level Fixed Effects 

Re-estimate the **Expanded Model** using fixed effects at the state level. 

- What do you estimate for coefficients on the blood alcohol variables? How do the coefficients on the blood alcohol variables change, if at all? 
- What do you estimate for coefficients on per se laws? How do the coefficients on per se laws change, if at all? 
- What do you estimate for coefficients on primary seat-belt laws? How do the coefficients on primary seatbelt laws change, if at all? 

Which set of estimates do you think is more reliable? Why do you think this? 

- What assumptions are needed in each of these models?  
- Are these assumptions reasonable in the current context?

## Answer:

From the comparison below we can see:

1. The coefficients for *blood alcohol limit* is -0.02 for limit at 0.08 and -0.16 for limit at 0.1, both less negative than those in the pooled model. Unlike the pooled model, the blood_alcohol_limit are not significant.

2. The coefficient for *per se law* is -0.053, which becomes significant and more negative in the FE model, suggesting adopting the per se law helps reduce the total fatalities rate by nearly 5%. (note that we approximate the log change with percent change)

3. The coefficient for *primary seat belt law* is -0.04, which also becomes significant and more negative in the FE model, implying the primary seat belt law can help reduce  the total fatalities rate by nearly 4%.

```{r} 
fixed_effect_model = plm(
  log_total_fatalities_rate ~ year + blood_alcohol_limit + per_se_law +
    seat_belt + speed_limit_70plus + graduated_drivers_license_law + perc14_24 +
    log_unemployment_rate + log_vehicle_miles_per_capita,
  data = data_clean,
  index = c("state", "year"),
  model = "within"
)
stargazer(
  expanded_model,
  fixed_effect_model,
  type = "text",
  omit = c("year"),
  omit.labels = c("year dummy variables"),
  column.labels = c("Pooled", "Fixed Effect")
)
```

For pooled OLS to be the appropriate estimator, we need to assume:

1- **Linearity**: the model is linear in parameters

2- **I.I.D.** : The observations are independent across individuals but not necessarily across time.

3- **Indentifiability**: the regressors, including a constant, are not perfectly colinear, and all regressors (but the constant) have non-zero variance and not too many extreme values.

4- The independent variables are **uncorrelated** with idiosyncratic error term and individual-specific effect.

The pooled OLS estimator is consistent under assumptions 1-4. We also need to assume **homoskedasticity** and **no serial correlation** in the data to do inference based on the conventional OLS estimator of the covariance matrix.

The main issues of pooled OLS is when the unobserved individual-specific effects are correlated with the independent variables, the model will suffer from an omitted variable bias. In this case, the fixed effect (FE) model is preferred because it eliminates the unobserved time-invariant effects by de-mean procedures. The rest of the assumptions for the FE model are the same.

We can run a test to see whether the pooled OLS model is better than the FE model as below. The null hypothesis is rejected, suggesting the significance of individual fixed effects. **Therefore, the FE model provides better estimates**.

```{r}
pFtest(fixed_effect_model, expanded_model)
```


# (10 points) Consider a Random Effects Model 

Instead of estimating a fixed effects model, should you have estimated a random effects model?

- Please state the assumptions of a random effects model, and evaluate whether these assumptions are met in the data. 
- If the assumptions are, in fact, met in the data, then estimate a random effects model and interpret the coefficients of this model. Comment on how, if at all, the estimates from this model have changed compared to the fixed effects model. 
- If the assumptions are **not** met, then do not estimate the data. But, also comment on what the consequences would be if you were to *inappropriately* estimate a random effects model. Would your coefficient estimates be biased or not? Would your standard error estimates be biased or not? Or, would there be some other problem that might arise?

## Answer:

The random effects (RE) model assumes the time-invariant unobserved effect is uncorrelated with the explanatory variables. Other than that, the assumptions are the same as the FE model. Whether the time-invariant unobserved effect is uncorrelated with the explanatory variables can be tested with a Hausman test as below. **We reject the null hypothesis, suggesting the assumption is not met.** ^[We notice that if we use the original total fatalities rate instead of its logged version, the test will fail to reject the null hypothesis. Therefore, the data transformation can also influence the validity of the model choice.] Since the individual specific effects exist and they are correlated with the explanatory variables, the random effect model will suffer from biased coefficient estimates and standard error estimates.

```{r}
random_effect_model = plm(
  log_total_fatalities_rate ~ year + blood_alcohol_limit + per_se_law +
    seat_belt + speed_limit_70plus + graduated_drivers_license_law + perc14_24 +
    log_unemployment_rate + log_vehicle_miles_per_capita,
  data = data_clean,
  index = c("state", "year"),
  model = "random"
)
phtest(fixed_effect_model,random_effect_model)
```


# (10 points) Model Forecasts 

The COVID-19 pandemic dramatically changed patterns of driving. Find data (and include this data in your analysis, here) that includes some measure of vehicle miles driven in the US. Your data should at least cover the period from January 2018 to as current as possible. With this data, produce the following statements: 

- Comparing monthly miles driven in 2018 to the same months during the pandemic: 
  - What month demonstrated the largest decrease in driving? How much, in percentage terms, lower was this driving? 
  - What month demonstrated the largest increase in driving? How much, in percentage terms, higher was this driving? 
  
Now, use these changes in driving to make forecasts from your models. 

- Suppose that the number of miles driven per capita, increased by as much as the COVID boom. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.
- Suppose that the number of miles driven per capita, decreased by as much as the COVID bust. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.

## Answer

We download both the monthly *Vehicle Miles Traveled* and *Population* data of the U.S. from [FRED](https://fred.stlouisfed.org/) and use their ratio as the vehicle miles traveled(VMT) per capita, as below. The time series plot shows it has strong seasonality and changed dramatically during the pandemic.

```{r fig.height=3}
if(!"fredr"%in%rownames(installed.packages())) {install.packages("fredr")}
library(fredr)
fredr_set_key("cd565a10e83d56f9f1150d5a2c067e2a")
# Vehicle Miles Traveled are in Millions of Miles, Not Seasonally Adjusted
vmt=fredr(
  series_id = "TRFVOLUSM227NFWA",
  observation_start = as.Date("1990-01-01"),
  observation_end = as.Date("2023-05-01")
) %>% dplyr::select(date,value) %>% as_tsibble(index=date)

# population are in Thousands, Not Seasonally Adjusted
us_pop=fredr(
  series_id = "POPTHM",
  observation_start = as.Date("1990-01-01"),
  observation_end = as.Date("2023-05-01")
)%>% dplyr::select(date,value) %>% as_tsibble(index=date)
# vmt_per_capita are in miles per capita
vmt_per_capita <- vmt %>% mutate(value=value/us_pop$value*1000,date=yearmonth(date)) %>% 
  rename(vehicle_miles_per_capita=value)
vmt_per_capita %>% ggplot(aes(x=date,y=vehicle_miles_per_capita))+geom_line()+
  ggtitle("Monthly VMT per capita")
```

We compare monthly miles driven during pandemic with the same month of 2018. As the table and plot below show:[we exclude the data of Jan 2020 and Feb 2020 because the pandemic hadn't started in the US then.]

  - Apr 2020 demonstrated the largest decrease in driving by 39.69% 
  - Sep 2022 demonstrated the largest increase in driving by 0.69% 

```{r fig.height=3}
vmt_pivot = vmt_per_capita %>% 
  mutate(year = year(date), month = month(date)) %>% as_tibble() %>%
  pivot_wider(id_cols = -date,
              names_from = year,
              values_from = vehicle_miles_per_capita)
comparison = vmt_pivot %>% mutate(
  d2020 = 100 * (.data[["2020"]] / .data[["2018"]] - 1),
  d2021 = 100 * (.data[["2021"]] / .data[["2018"]] - 1),
  d2022 = 100 * (.data[["2022"]] / .data[["2018"]] - 1),
  d2023 = 100 * (.data[["2023"]] / .data[["2018"]] - 1),
) %>% dplyr::select(month, d2020, d2021, d2022, d2023)
comparison[1:2,2]=NA
comparison %>% pivot_longer(cols = c(d2020, d2021, d2022, d2023)) %>%
  ggplot(aes(x = month, y = value, color = name)) + geom_line()+
  ggtitle("Percent change of VMT per capita:\nduring pandemic vs. the same month in 2018")+
  ylab("%")
```


```{r}
kable(comparison,digits = 2,
      caption = "Percent change of VMT per capita over the same month of 2018")
```

In the FE model above, the coefficient of *log_vehicle_miles_per_capita* is 0.678, which means a unit increase in *log_vehicle_miles_per_capita* will cause the *log_total_fatalities_rate* to increase by 0.678. Therefore, we can calculate the changes to the total fatalities rate based on the changes of *log_vehicle_miles_per_capita*  during boom and bust in the pandemic, as below. However, since each state has different population and fatalities rate, we cannot calculate a specific number of fatalities based on the data above.

```{r}
comparison_log = vmt_pivot %>% mutate(
  d2020 = log(.data[["2020"]]) - log(.data[["2018"]]),
  d2021 = log(.data[["2021"]]) - log(.data[["2018"]]),
  d2022 = log(.data[["2022"]]) - log(.data[["2018"]]),
  d2023 = log(.data[["2023"]]) - log(.data[["2018"]]),
) %>% dplyr::select(month, d2020, d2021, d2022, d2023)
result = data.frame(
  scenario = c("boom", "bust"),
  time = c("2022M09", "2020M04"),
  change_in_log_vmt_per_capita = 
    c(as.numeric(comparison_log[9, "d2022"]), 
      as.numeric(comparison_log[4, "d2020"]))
) %>% mutate(
  change_in_log_total_fatalities_rate = change_in_log_vmt_per_capita *0.678)
require(knitr)
kable(result, digits = 2)
```


# (5 points) Evaluate Error 

If there were serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors? Is there any serial correlation or heteroskedasticity? 

## Answer

Serial correlation or heteroskedasticity will not influence the consistence of the estimators but will lead to inaccurate estimate of their standard errors.

We can use Breusch Pagan test the FE model (the only valid one) for heteroskedasticity. As shown below, the test rejects the null hypothesis of homoskedasticity, in favor of heteroskedasticity. Therefore, we should use robust standard error estimates.

```{r}
pcdtest(fixed_effect_model, test = "lm")
```

We perform both Durbin Watson test and Breusch-Godfrey test for the FE model. As shown below, both tests suggest serial correlation in errors. This confirms that we should use robust standard error estimates.

```{r}
pdwtest(fixed_effect_model)
pbgtest(fixed_effect_model, order = 2)
```

We chose to use Arrellano standard errors because they are the most robust to both heteroskedasticity and serial correlation. The models are compared with Arrellano SE in the stargazer table below to conclude our analysis.

```{r warning=FALSE}
# if heteroskadasticity AND serial autocorrelation,
# use arrellano standard errors
fe_arrellano.se <- sqrt(diag(vcovHC(fixed_effect_model, method="arellano", type="HC0")))
re_arrellano.se <- sqrt(diag(vcovHC(random_effect_model, method="arellano", type="HC0")))
pooled_arrellano.se <- sqrt(diag(vcovHC(expanded_model, method="arellano", type="HC0")))

stargazer(
  expanded_model,
  fixed_effect_model,
  random_effect_model,
  se=list(pooled_arrellano.se,fe_arrellano.se, re_arrellano.se), # must be list
  type = "text",
  omit = c("year"),
  omit.labels = c("year dummy variables"),
  column.labels = c("Pooled", "Fixed Effects", "Random Effects")
)
```

